{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nothing But NumPy: A 2-layer Binary Classification Neural Network on Iris Flowers\n",
    "\n",
    "Part of the blog [\"Nothing but NumPy: Understanding & Creating Binary Classification Neural Networks with Computational Graphs fromÂ Scratch\"](https://medium.com/@rafayak/nothing-but-numpy-understanding-creating-binary-classification-neural-networks-with-e746423c8d5c)- by [Rafay Khan](https://twitter.com/RafayAK)\n",
    "\n",
    "In this notebook we'll create a 2-layer neural network (i.e. one input and one output layer) and train it on the eintire Iris dataset to classify **Iris-virginica vs. others**\n",
    "\n",
    "First, let's import NumPy, our neural net Layers, the Binary Cross-Entropy(bce) Cost function and helper functions.\n",
    "\n",
    "_Feel free to look into the helper functions in the utils directory._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Layers.LinearLayer import LinearLayer\n",
    "from Layers.ActivationLayer import SigmoidLayer\n",
    "from util.utilities import *\n",
    "from util.cost_functions import compute_stable_bce_cost\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to show all the generated plots inline in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/verg_vs_other.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience we'll load the data through [scikit-learn](https://scikit-learn.org/stable/index.html#). \n",
    "\n",
    "_If you don't have it installed please refer to this [link](https://scikit-learn.org/stable/install.html)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from scikit-learn's datasets module\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()  # returns a python dictionary with the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the dataset contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **data**: contains the 4 features of each example in a row, has 150 rows\n",
    "- **target**: contains the label for each example _(0->setosa, 1->versicolor, 2->virginica)_\n",
    "- **target_names**: contains the names of each target label\n",
    "- **DESCR**: contains the desription of the dataset\n",
    "- **feature_names**: contains the names of the 4 features(sepal length, sepal width, petal length, petal width)\n",
    "- **filename** : where the file is located on the computer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data.shape  # rows(examples), cols(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target.shape # labels for 150 flowers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names  # print the name of the 3 labels(species) an example could belong to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names  # name of each feature in data's columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "       [ 4.9,  3. ,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.3,  0.2],\n",
       "       [ 4.6,  3.1,  1.5,  0.2],\n",
       "       [ 5. ,  3.6,  1.4,  0.2]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data[:5, :]  # print first 5 examples from the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target[:5]  # print labels for the first 5 examples in the Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the data of the **first** 5 examples looks as follows:\n",
    "\n",
    "| exmaple# | sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | target | target name|\n",
    "| --- | --- | --- || --- | --- | --- |\n",
    "| 0 | 5.1 | 3.5 | 1.4 |  0.2| 0|  setosa\n",
    "| 1 |4.9|  3. |  1.4|  0.2|0|  setosa\n",
    "| 2 |4.7|  3.2|  1.3|  0.2|0|  setosa\n",
    "| 3 |4.6|  3.1|  1.5|  0.2|0|  setosa\n",
    "| 4 |5. |  3.6|  1.4|  0.2|0|  setosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to fix the shape of the target array as a precaution so that it's shape matches the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X(input) will be entire training data\n",
    "X = iris.data \n",
    "\n",
    "# fix the labes shape so that instead of (150,) its (150,1),\n",
    "# helps avoiding weird broadcasting errors\n",
    "Y = (iris.target).reshape((150, 1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice** in the table above that the first 5 examples belong to __'setosa'__ species, this pattern continues in the dataset(the pattern is all _setosa_ examples followed by _versicolor_ examples and finally _virginica_ examples). ___A good practice is to randomize the data before training a neural network, so that the neural network does not, by accident, learn a trivial ordering pattern in the data.___\n",
    "\n",
    "So let's randomize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(48)  # for reproducible randomization \n",
    "random_indices = np.random.permutation(len(X))  # genrate random permutation of indices\n",
    "\n",
    "X_train = X[random_indices]\n",
    "Y_train = Y[random_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's again print the first 5 examples and see the results(note this time features are only two - petal lenght, petal width )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.7,  2.9,  4.2,  1.3],\n",
       "       [ 6.1,  2.8,  4.7,  1.2],\n",
       "       [ 6.1,  2.6,  5.6,  1.4],\n",
       "       [ 4.5,  2.3,  1.3,  0.3],\n",
       "       [ 5.9,  3.2,  4.8,  1.8]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the data of the **first** 5 examples looks as follows:\n",
    "\n",
    "| exmaple# | sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | target | target name|\n",
    "| --- | --- | --- || --- | --- | --- |\n",
    "| 0 | 5.7|  2.9|  4.2|  1.3| 1|  versicolor\n",
    "| 1 | 6.1|  2.8|  4.7|  1.2|1|  versicolor\n",
    "| 2 |6.1 |  2.6|  5.6|  1.4|2|  virginica\n",
    "| 3 |4.5 |  2.3|  1.3|  0.3|0|  setosa\n",
    "| 4 | 5.9|  3.2|  4.8|  1.8|1|  versicolor\n",
    "\n",
    "\n",
    "Finally, let's put training set(`X_train`)  & and labels(`Y_train`) in the correct shape `(feat, examples)` and  `(examples,1)`, respectively. Also we'll make the target label ___virginica=1___ and the rest ___0___.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the data so that it's in the correct shape \n",
    "# for passing through neural network\n",
    "# also binarize the classes viginica=1 and the rest 0\n",
    "X_train = X_train.T\n",
    "Y_train = Y_train.T  \n",
    "Y_train = (Y_train==2).astype('int')  # uses bool logic  to binarize labels, wherever label=2 output True(1) rest Flase(0)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data, X_train: (4, 150)\n",
      "Shape of  labels, Y_train: (1, 150)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of training data, X_train: {}\".format(X_train.shape))\n",
    "print(\"Shape of  labels, Y_train: {}\".format(Y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:, :5]  # print first five examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of features are 4 in this dataset we cannot visuallize it on a 2-D plotðŸ˜’ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we are ready to setup and train the Neural Network\n",
    "\n",
    "This is the neural net architecture we'll use\n",
    "\n",
    "![](imgs/4in_2_layer_NN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training constants\n",
    "learning_rate = 0.3\n",
    "number_of_epochs = 5000\n",
    "\n",
    "np.random.seed(48) # set seed value so that the results are reproduceable\n",
    "                  # (weights will now be initailzaed to the same pseudo-random numbers, each time)\n",
    "\n",
    "\n",
    "# Our network architecture has the shape: \n",
    "#                   (input)--> [Linear->Sigmoid] -> [Linear->Sigmoid] -->(output)  \n",
    "\n",
    "#------ LAYER-1 ----- define hidden layer that takes in training data \n",
    "Z1 = LinearLayer(input_shape=X_train.shape, n_out=3, ini_type='xavier')\n",
    "A1 = SigmoidLayer(Z1.Z.shape)\n",
    "\n",
    "#------ LAYER-2 ----- define output layer that takes in values from hidden layer\n",
    "Z2= LinearLayer(input_shape=A1.A.shape, n_out= 1, ini_type='xavier')\n",
    "A2= SigmoidLayer(Z2.Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch#0: 0.5842834152498868\n",
      "Cost at epoch#100: 0.36310976755875046\n",
      "Cost at epoch#200: 0.2312121547146245\n",
      "Cost at epoch#300: 0.20815465292508076\n",
      "Cost at epoch#400: 0.15236070381583322\n",
      "Cost at epoch#500: 0.12396740039354323\n",
      "Cost at epoch#600: 0.10787788203569885\n",
      "Cost at epoch#700: 0.09750860915431853\n",
      "Cost at epoch#800: 0.09028448866093153\n",
      "Cost at epoch#900: 0.08497225692281726\n",
      "Cost at epoch#1000: 0.08090311871991471\n",
      "Cost at epoch#1100: 0.07768356139449455\n",
      "Cost at epoch#1200: 0.07506790890045067\n",
      "Cost at epoch#1300: 0.07289557330707988\n",
      "Cost at epoch#1400: 0.07105762631932071\n",
      "Cost at epoch#1500: 0.06947787956085147\n",
      "Cost at epoch#1600: 0.06810164192309977\n",
      "Cost at epoch#1700: 0.06688876399133185\n",
      "Cost at epoch#1800: 0.06580918596634044\n",
      "Cost at epoch#1900: 0.06484000390592067\n",
      "Cost at epoch#2000: 0.06396348719044047\n",
      "Cost at epoch#2100: 0.06316570898725746\n",
      "Cost at epoch#2200: 0.06243558167500952\n",
      "Cost at epoch#2300: 0.06176416575211792\n",
      "Cost at epoch#2400: 0.06114416711531772\n",
      "Cost at epoch#2500: 0.06056956640019008\n",
      "Cost at epoch#2600: 0.06003534239021894\n",
      "Cost at epoch#2700: 0.05953726338953337\n",
      "Cost at epoch#2800: 0.05907172832083099\n",
      "Cost at epoch#2900: 0.05863564460852778\n",
      "Cost at epoch#3000: 0.058226333536848314\n",
      "Cost at epoch#3100: 0.05784145629892699\n",
      "Cost at epoch#3200: 0.05747895573787877\n",
      "Cost at epoch#3300: 0.057137010059214795\n",
      "Cost at epoch#3400: 0.056813995720735394\n",
      "Cost at epoch#3500: 0.056508457384639124\n",
      "Cost at epoch#3600: 0.05621908331717527\n",
      "Cost at epoch#3700: 0.05594468499196455\n",
      "Cost at epoch#3800: 0.05568417992764552\n",
      "Cost at epoch#3900: 0.05543657699232723\n",
      "Cost at epoch#4000: 0.05520096355297109\n",
      "Cost at epoch#4100: 0.054976493948822265\n",
      "Cost at epoch#4200: 0.05476237883213468\n",
      "Cost at epoch#4300: 0.054557874951638094\n",
      "Cost at epoch#4400: 0.05436227495726613\n",
      "Cost at epoch#4500: 0.054174896780065686\n",
      "Cost at epoch#4600: 0.05399507209030688\n",
      "Cost at epoch#4700: 0.05382213326340516\n",
      "Cost at epoch#4800: 0.0536553981990101\n",
      "Cost at epoch#4900: 0.053494152274254206\n"
     ]
    }
   ],
   "source": [
    "costs = [] # initially empty list, this will store all the costs after a certian number of epochs\n",
    "\n",
    "# Start training\n",
    "for epoch in range(number_of_epochs):\n",
    "    \n",
    "    # ------------------------- forward-prop -------------------------\n",
    "    Z1.forward(X_train)\n",
    "    A1.forward(Z1.Z)\n",
    "    \n",
    "    Z2.forward(A1.A)\n",
    "    A2.forward(Z2.Z)\n",
    "    \n",
    "    # ---------------------- Compute Cost ----------------------------\n",
    "    cost, dZ2 = compute_stable_bce_cost(Y=Y_train, Z=Z2.Z)\n",
    "    \n",
    "    # print and store Costs every 100 iterations and of the last iteration.\n",
    "    if (epoch % 100) == 0:\n",
    "        print(\"Cost at epoch#{}: {}\".format(epoch, cost))\n",
    "        costs.append(cost)\n",
    "    \n",
    "    # ------------------------- back-prop ----------------------------\n",
    "    \n",
    "    Z2.backward(dZ2)\n",
    "    \n",
    "    A1.backward(Z2.dA_prev)\n",
    "    Z1.backward(A1.dZ)\n",
    "    \n",
    "    # ----------------------- Update weights and bias ----------------\n",
    "    Z2.update_params(learning_rate=learning_rate)\n",
    "    Z1.update_params(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well the neural net peforms on the training data after the training as finished\n",
    "\n",
    "`predict` helper functionin the cell below returns three things:\n",
    "\n",
    "* `p`: predicted labels (output 1 if predictded output is greater than classification threshold `thresh`)\n",
    "* `probas`: raw probabilities (how sure the neural net thinks the output is 1, this is just `P_hat`)\n",
    "* `accuracy`: the number of correct predictions from total predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted outputs of first 5 examples: \n",
      "[[ 0.  0.  1.  0.  0.]]\n",
      "The predicted prbabilities of first 5 examples:\n",
      " [[ 0.001  0.002  0.981  0.001  0.407]]\n",
      "\n",
      "The accuracy of the model is: 98.66666666666666%\n"
     ]
    }
   ],
   "source": [
    "classifcation_thresh = 0.5\n",
    "\n",
    "\n",
    "predicted_outputs, p_hat, accuracy = predict(X=X_train, Y=Y_train, \n",
    "                                             Zs=[Z1, Z2], As=[A1, A2], thresh=classifcation_thresh)\n",
    "\n",
    "print(\"The predicted outputs of first 5 examples: \\n{}\".format(predicted_outputs[:,:5]))\n",
    "print(\"The predicted prbabilities of first 5 examples:\\n {}\".format(np.round(p_hat[:, :5], decimals=3)) )\n",
    "print(\"\\nThe accuracy of the model is: {}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curve(costs, learning_rate, total_epochs=number_of_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounus \n",
    "\n",
    "Try different learning rates. See the effect on learning curve. \n",
    "\n",
    "_(Hint: if the learning curve is not smooth and very erratic then Gradient Descent is bouncing around the minimum point, all because the learnig rate is too high.)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
